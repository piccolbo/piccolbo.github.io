---
layout: post
title: "A different complexity class for map reduce computations"
date: 2010-07-01
comments: false
---

<div class='post'>
Computer theory is taking notice of the popularity and importance of the map reduce model of computation, but the first related computational complexity class to appear in the literature, despite being carefully motivated, doesn't capture the requirements of everyday problem solving.<br />For short, I don't think it's parallel enough.<br /><br /><span class="fullpost">  </span><br /><br />The linked article "A Model of computation for map reduce", states that assuming a number of processors that is linear in the size of the input is not practical, but allows for polynomial computing time, which is of course not practical for internet sized data sets. I think that to connect practice with asymptotic analysis in a meaningful way we need to look at growth rates. If today it takes google 15 minutes to refresh their index, or 20 minutes for Yahoo to update their spam filter, what is going to happen next month with a index size 20% larger or a spam volume increased by as much? We could replace all the disks with disks that are 20% larger, the same with memory size and processor speed and indeed machines are continuously updated in large clusters with more powerful ones. But we could have maxed out on processor type, number and size of spindles or RAM modules. In response to fast growth, what one can do is just buy more machines. The degree of parallelism with modern clusters and map reduce is the most pliable of dimensions, linearly related to the amount of money available. Increasing processor speed and the like needs to wait for technology developments outside the control of a company and definitely slower than provisioning more machines, in particular in compute clouds where the number of machines is a parameter to an API call. On the other hand, waiting more is seldom an option: Google is unlikely to trade index size with freshness and Yahoo needs to process its spam before it gets to people's inboxes, not to mention the millisecond requirements of the financial industry. In this hurried, real time world with huge data sets, polynomial time is too slow. Therefore my proposal is to allow for a linear number of machines, while assuming constant restrictions on per-machine resources, like RAM, disk and of course speed. As far as time, taking a page from the NC class I would suggest a polylog bound, meaning that we accept a modest slow down for larger problem sizes. Even that might be unacceptable in some application, and an alternate constant bound for very time-critical applications is an option, but also a very restrictive one. Since the map reduce model at the moment requires the use of disk storage, a separate bound on the number of rounds seems necessary and could be polylog as well. I am not sure what the acronym could be for such a class or what its relation is with other classes, but I know it is's been an effective conceptual guide for my map reduce algorithm work. As the size of clusters increases, as does the number of cores per processor, I think we need to embrace a higher degree of parallelism than was practical even a few years ago and that should be reflected in the definition of complexity classes if theory and practice are to advance together.</div>
